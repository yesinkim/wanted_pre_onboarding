{"cells":[{"cell_type":"markdown","metadata":{"id":"592U6lXs3d2t"},"source":["# Week2_2 Assignment\n","\n","## [BASIC](#Basic) \n","- \"네이버 영화 감성 분류\" 데이터를 불러와 `pandas` 라이브러리를 사용해 **전처리** 할 수 있다.\n","- 적은 데이터로도 높은 성능을 내기 위해, pre-trained `BERT` 모델 위에 1개의 hidden layer를 쌓아 **fine-tuning**할 수 있다.\n","\n","## [CHALLENGE](#Challenge)\n","- 토큰화된 학습 데이터를 배치 단위로 갖는 **traindata iterator**를 구현할 수 있다. \n","\n","## [ADVANCED](#Advanced)\n","- **loss와 optimizer 함수**를 사용할 수 있다. \n","- traindata iterator를 for loop 돌며 **fine-tuning** 할 수 있다.\n","- fine-tuning의 2가지 방법론을 비교할 수 있다. \n","  - BERT 파라미터를 **freeze** 한 채 fine-tuning (Vision에서 주로 사용하는 방법론)\n","  - BERT 파라미터를 **unfreeze** 한 채 fine-tuning (NLP에서 주로 사용하는 방법론)\n","\n","\n","### Reference\n","- [huggingface 한국어 오픈소스 모델](https://huggingface.co/models?language=ko&sort=downloads&search=bert)\n","- [transformer BertForSequenceClassification 소스 코드](https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L1501)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KSX-wQA1RD1h","executionInfo":{"status":"ok","timestamp":1646136474753,"user_tz":-540,"elapsed":6635,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","import torch\n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4Reyt-HvLnJv","executionInfo":{"status":"ok","timestamp":1646136474754,"user_tz":-540,"elapsed":13,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["# seed\n","seed = 7777\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gUR6vb3L3d2u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136474756,"user_tz":-540,"elapsed":13,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"e7032184-01ab-4a3a-be73-59cdef01e819"},"outputs":[{"output_type":"stream","name":"stdout","text":["# available GPUs : 1\n","GPU name : Tesla K80\n","cuda\n"]}],"source":["# device type\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","  print(f\"# available GPUs : {torch.cuda.device_count()}\")\n","  print(f\"GPU name : {torch.cuda.get_device_name()}\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"c93M8XmjLnJw"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"0REKl4EvT9G1"},"source":["### 데이터 다운로드 및 DataFrame 형태로 불러오기\n","- 내 구글 드라이브에 데이터를 다운받은 후 코랩에 드라이브를 마운트하면 데이터를 영구적으로 사용할 수 있음.\n","- [네이버영화감성분류](https://github.com/e9t/nsmc)\n","  - trainset: 150,000 \n","  - testset: 50,000 "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"lEWUggR1R9rS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136500576,"user_tz":-540,"elapsed":25830,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"3ddae94f-900d-47d8-db05-719f8744d309"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rov1s8IxSLqy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136500577,"user_tz":-540,"elapsed":30,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"4959ce29-022c-45c2-982d-c9b76116fc53"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}],"source":["cd \"/content/drive/MyDrive/\""]},{"cell_type":"code","execution_count":6,"metadata":{"id":"OjPGnbEjVYmj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136500578,"user_tz":-540,"elapsed":24,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"c71bf538-bca1-4399-998a-d0efa52e1aa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'nsmc' already exists and is not an empty directory.\n"]}],"source":["# 데이터 다운로드\n","!git clone https://github.com/e9t/nsmc.git"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SueG9v14YbgF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136500579,"user_tz":-540,"elapsed":17,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"2d89eaa7-8bcb-4699-9f21-e9e8f906ede7"},"outputs":[{"output_type":"stream","name":"stdout","text":["My current directory : /content/drive/MyDrive\n"]}],"source":["_CUR_DIR = os.path.abspath(os.curdir)\n","print(f\"My current directory : {_CUR_DIR}\")\n","_DATA_DIR = os.path.join(_CUR_DIR, \"nsmc\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9J6KQ8dzaHBi","executionInfo":{"status":"ok","timestamp":1646136501763,"user_tz":-540,"elapsed":1197,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["# nsmc/ratings_train.txt를 DataFrame 형태로 불러오기\n","df = pd.read_csv('/content/drive/MyDrive/nsmc/ratings_train.txt', sep='\\t')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"3cUsoBEPahlo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136501764,"user_tz":-540,"elapsed":29,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"ce183c7d-d258-4d35-ce9d-ceb3d3559fa8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150000, 3)"]},"metadata":{},"execution_count":9}],"source":["# 데이터 크기 확인\n","df.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Ic3k9CORaXzM","colab":{"base_uri":"https://localhost:8080/","height":267},"executionInfo":{"status":"ok","timestamp":1646136501765,"user_tz":-540,"elapsed":27,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"95ca9987-dcbe-43e6-b61d-5d06f0077cc1"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-f4561d5d-c59a-4eb9-8786-15bafcb44aa9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4561d5d-c59a-4eb9-8786-15bafcb44aa9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f4561d5d-c59a-4eb9-8786-15bafcb44aa9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f4561d5d-c59a-4eb9-8786-15bafcb44aa9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"]},"metadata":{},"execution_count":10}],"source":["# 데이터 일부 확인\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"JA1F0tHWLnJz"},"source":["### 데이터 결측치 제거 및 데이터 수 줄이기 \n","- 학습 데이터 수는 150,000개로 매우 많은 양이다. 하지만 우리가 실생활에서 마주할 데이터는 이렇게 많지 않다. 이 때 유용하게 사용되는 것이 **fine-tuning** 학습 방법이다.   \n","- Fine-tuning은 단어의 의미를 이미 충분히 학습한 모델 (여기서는 **BERT**)을 가져와 그 위에 추가적인 Nueral Network 레이어를 쌓은 후 학습하는 방법론이다. 이미 BERT가 단어의 의미를 충분히 학습했기 때문에 **적은 데이터**로 학습해도 우수한 성능을 낼 수 있다는 장점이 있다. \n","- **데이터의 label의 비율이 5:5를 유지하면서** 학습 데이터 수를 150,000개에서 1,000개로 줄이\b는 함수 `label_evenly_balanced_dataset_sampler`를 구현하라.\n","  - 함수 정의 \n","    - 입력 매개변수\n","      - df : DataFrame\n","      - n_sample : df에서 샘플링할 row의 개수 (여기서는 1000개로 정의한다)\n","    - 조건\n","      - label의 비율이 5:5를 유지할 수 있도록 샘플링한다.\n","    - 반환값\n","      - row의 개수가 1000개인 dataframe"]},{"cell_type":"code","source":["# 결측치 개수\n","df.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTtd6iiWnHHo","executionInfo":{"status":"ok","timestamp":1646136501765,"user_tz":-540,"elapsed":24,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"766fe97f-7438-4dcb-d716-25e7f9c9ecdd"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["id          0\n","document    5\n","label       0\n","dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Lh9BSiSeMms7","executionInfo":{"status":"ok","timestamp":1646136501766,"user_tz":-540,"elapsed":20,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["# df에서 결측치 (na 값) 제거\n","\n","df = df.dropna()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ommF5KH4akCJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136501767,"user_tz":-540,"elapsed":20,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"67bf4d9b-f257-44c5-c734-3da14ab56c53"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    75170\n","1    74825\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":13}],"source":["# label별 데이터 수 확인\n","# pandas의 value_counts 함수 활용\n","# 0 -> 부정 1 -> 긍정\n","\n","df['label'].value_counts()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"_ii06wCsc107","executionInfo":{"status":"ok","timestamp":1646136501768,"user_tz":-540,"elapsed":18,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["# 학습 데이터 샘플 개수 설정\n","\n","n_sample = 1000"]},{"cell_type":"code","source":["df[df['label']==0].sample(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"UCrg5C5DuDBk","executionInfo":{"status":"ok","timestamp":1646136501769,"user_tz":-540,"elapsed":18,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"16101657-f1c1-466f-ea15-00d5401ae8f5"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-6c9acc6a-ae1b-4201-9267-42404375a1f0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>112342</th>\n","      <td>8644158</td>\n","      <td>그냥저냥... 엔딩개노답</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>144144</th>\n","      <td>5494075</td>\n","      <td>주성치 영화를 보는 듯 했지만, 아직 많이 멀었다</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c9acc6a-ae1b-4201-9267-42404375a1f0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6c9acc6a-ae1b-4201-9267-42404375a1f0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6c9acc6a-ae1b-4201-9267-42404375a1f0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["             id                     document  label\n","112342  8644158                그냥저냥... 엔딩개노답      0\n","144144  5494075  주성치 영화를 보는 듯 했지만, 아직 많이 멀었다      0"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Hrkhl69Dc-kr","executionInfo":{"status":"ok","timestamp":1646136502367,"user_tz":-540,"elapsed":615,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["# 샘플링 함수 구현\n","# random 모듈에서 제공되는 함수 활용\n","# input: 학습 데이터 샘플 개수\n","# output: 샘플링 데이터\n","\n","\n","def label_evenly_balanced_dataset_sampler(df, sample_size):\n","  \"\"\"\n","  데이터 프레임의을 sample_size만큼 임의 추출해 새로운 데이터 프레임을 생성.\n","  이 때, \"label\"열의 값들이 동일한 비율을 갖도록(5:5) 할 것.\n","  \"\"\"\n","  df0 = df[df['label']==0]\n","  df1 = df[df['label']==1]\n","\n","\n","  random0 = df0.sample(int(sample_size/2))\n","  random1 = df1.sample(int(sample_size/2))\n","\n","  sample = pd.concat([random0, random1])\n","\n","  return sample\n","\n","sample_df = label_evenly_balanced_dataset_sampler(df, n_sample)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"hXLT6tAdaA34","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136502368,"user_tz":-540,"elapsed":15,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"69c382ad-742b-4ad6-ccff-0aa0205fda38"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    500\n","1    500\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":17}],"source":["# 검증\n","\n","sample_df.label.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"cLNUjgawLnJ1"},"source":["### CustomClassifier 클래스 구현\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_bertclf.png?raw=true\" width=400>\n","\n","- 그림과 같이 사전 학습(pre-trained)된 `BERT` 모델을 불러와 그 위에 **1 hidden layer**와 **binary classifier layer**를 쌓아 fine-tunning 모델을 생성할 것이다.    \n","---\n","- hidden layer 1개와 output layer(binary classifier layer)를 갖는 `CustomClassifier` 클래스를 구현하라.\n","- 클래스 정의\n","  - 생성자 입력 매개변수\n","    - `hidden_size` : BERT의 embedding size\n","    - `n_label` : class(label) 개수\n","  - 생성자에서 생성할 변수\n","    - `bert` : BERT 모델 인스턴스 \n","    - `classifier` : 1 hidden layer + relu +  dropout + classifier layer를 stack한 `nn.Sequential` 모델\n","      - 첫번재 히든 레이어 (첫번째 `nn.Linear`)\n","        - input: BERT의 마지막 layer의 1번재 token ([CLS] 토큰) (shape: `hidden_size`)\n","        - output: (shape: `linear_layer_hidden_size`)\n","      - 아웃풋 레이어 (두번째 `nn.Linear`)\n","        - input: 첫번째 히든 레이어의 아웃풋 (shape: `linear_layer_hidden_size`)\n","        - output: target/label의 개수 (shape:2)\n","  - 메소드\n","    - `forward()`\n","      - BERT output에서 마지막 레이어의 첫번째 토큰 ('[CLS]')의 embedding을 가져와 `self.classifier`에 입력해 아웃풋으로 logits를 출력함.\n","  - 주의 사항\n","    - `CustomClassifier` 클래스는 부모 클래스로 `nn.Module`을 상속 받는다.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"U0WbqVv62Zvy","executionInfo":{"status":"ok","timestamp":1646136502369,"user_tz":-540,"elapsed":14,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Im98H4-U1eQQ","executionInfo":{"status":"ok","timestamp":1646136502370,"user_tz":-540,"elapsed":14,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["# classifier 구현\n","class CustomClassifier(nn.Module):\n","\n","  def __init__(self, hidden_size: int, n_label: int):\n","    super(CustomClassifier, self).__init__()\n","\n","    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n","\n","    dropout_rate = 0.1\n","    linear_layer_hidden_size = 32\n","\n","    self.classifier = nn.Sequential(\n","        nn.Linear(hidden_size, linear_layer_hidden_size),\n","        nn.ReLU(),\n","        nn.Dropout(dropout_rate),\n","        nn.Linear(linear_layer_hidden_size, n_label)\n","    ) # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용\n","\n","\n","  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","\n","    outputs = self.bert(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","    )\n","\n","    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n","    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n","\n","    logits = self.classifier(cls_token_last_hidden_states)\n","\n","    return logits"]},{"cell_type":"markdown","metadata":{"id":"9x7PU1t1LnJ1"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"YXesCG5TLnJ1"},"source":["### 학습 데이터를 배치 단위로 저장하는 이터레이터 함수 `data_iterator` 구현\n","- 데이터 프레임을 입력 받아 text를 \b토큰 id로 변환하고 label은 텐서로 변환해 배치만큼 잘라 (input, \btarget) 튜플 형태의 이터레이터를 생성하는 `data_iterator` 함수를 구현하라.\n","- 함수 정의 \n","  - 입력 매개변수\n","    - `input_column` : text 데이터 column 명\n","    - `target_column` : label 데이터 column 명\n","    -  `batch_size` : 배치 사이즈\n","  - 조건\n","    - 함수는 다음을 수행해야 함 \n","      - 데이터 프레임 랜덤 셔플링\n","      - `tokenizer_bert`로 text를 token_id로 변환 + 텐서화 \n","      - target(label)을 텐서화\n","  - 반환값 \n","    - (input, target) 튜플 형태의 이터레이터를 반환"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"q-tJERGI4Fzk","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136511693,"user_tz":-540,"elapsed":9336,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"1692aac2-bb43-4741-9deb-521d6c6f01dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 11.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 35.4 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 26.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 41.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"JlcYCOyW3d2t","executionInfo":{"status":"ok","timestamp":1646136512310,"user_tz":-540,"elapsed":628,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"C_U_c-Mf3d2t","colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["3acfaa50c5304093b8e00203c6801a2a","7ab269b1881f4ab3b489469bd1245389","8076dae6044a45da92723900cda75ac8","1a4ef91de8744ff188dd6e30e9caebff","cf5d77276a9f42aa8eb135b446020af3","cf4635178eee419d9d95aaf12b870379","5eaea918576a45fb8c7bcc284ec06be2","4630aabfb7ca4257ab72ed623278c687","fae16c504b02402391b7ffcd554aab89","b50d556c91ed4ba684125bb285227fcf","4d04d42212664ea08123952e207534b3","bb89304cecf74e86ac220cb82fcea8d3","cc0dc80af7ab461da24ae4baf24820ae","89035877e6ea4a8c875b6ce3cd25b0da","4c7fd8263cba4634bb10276c71d7cef7","3f0d524e62c044d8b0d4bd68e4e7ca2f","67ce9c64c331433eb31cb8bb55568375","28b887641f624ad0b0e9410b1aafd53c","84fe90e99f3c437a9100795948228fa4","07c15b3a87794cfa8188e318099c1c3e","6c527225f7154eed86d647715c7d17a6","2621d38fad7d4169aad9d3b64dda54d1","d93a37fab7614aa383f17c15a7e9505d","0d42abb7209d4c7580964f079867f3cd","13571ef00ec34d078a1a29255d957be2","fe4fdabb9fc24384a080e17bbf5c4b99","05c08389cab0466e924690c3c52ca137","26825ab0ac884175adbe12b04eb7ec0b","f42b75d58d494247803eca3d9dec5372","85d3c2b434064f72af7cb335e2173529","8c5cd6a3b4604d5e87328c32b210ac22","ce661b2529c247bbb0ffe147791b24de","2b27da44ed76495e95dcf4d0e7366eeb","49a1001e923b4f1d83b0a0a4d03c3821","7c72ad75371242a8b20dc0257e7e02e8","c0702b08dd1d483f99b05bcc11280cbc","70213bd876234ba197d2376d35abea7b","de1c25e1d6e44148900768cb1921fce3","353fdbe6224c479bbfc64bbefbb762c7","67f40f9ec5a64e2f83c75d0c0542ccee","98f020424e1b4e888e866571e0505b91","29a89d9fb1e7437fb7981466930b89bc","0a139f2da1f34585927968051eb734d7","856f0caf33164b1295014de5036f0473","a5aa6b35f8d84c3aa8ecac05bd179490","acedf58755e84ae791a8924ff12748a3","6fcf3a85d02d412a8c4cb4d1fe24e1ea","eb6bd0883c56423880a3562e64d0ce3d","e925804e8cf1473cafa1b34aa684ff91","b8fc755368ac4054aabb2fddb15789c3","0f7035ebf2684efda505144482e84c7e","1396a7f597d94bb7b3776614e13bc463","b8a2ed76f5d045f38fbdcefd379efd1b","fd43ecd0a6e9452c8b51dd042f17c0aa","a55ea57425e445af98bd4c807825d6bd"]},"executionInfo":{"status":"ok","timestamp":1646136517410,"user_tz":-540,"elapsed":5109,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"a4a845ec-4a77-4ec3-8c96-cc9a4d1087f1"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3acfaa50c5304093b8e00203c6801a2a","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb89304cecf74e86ac220cb82fcea8d3","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d93a37fab7614aa383f17c15a7e9505d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49a1001e923b4f1d83b0a0a4d03c3821","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/483k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5aa6b35f8d84c3aa8ecac05bd179490","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"]},"metadata":{}}],"source":["tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\") # lower-cased version"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"p2VnIY-ALnJ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136517411,"user_tz":-540,"elapsed":123,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"0a538ce6-c53f-409b-abaf-814dce3d3d54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence: 호주판 로빈훗. 소름끼치도록 진실을 왜곡하다 .\n","\n","Tokenized Sentence: {'input_ids': tensor([[    2,  6248,  2025, 18512,  3519,    18, 16109,  2274,  2225, 16818,\n","          5352,  2069,  7399,  2205,  2062,    18,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}],"source":["# 토크나이징 예시 (1개의 문장)\n","\n","# 1. string type의 문장을 가져옴\n","ex_sent = sample_df.document.iloc[0]\n","print(f\"Original Sentence: {ex_sent}\\n\")\n","\n","# 2. 문장을 토크나이즈 함. 이 때, 특수 토큰 (\"[CLS]\", \"[SPE]\")을 자동으로 추가하고 pytorch의 tensor형태로 변환해 반환함\n","tensor_sent = tokenizer_bert(\n","    ex_sent,\n","    add_special_tokens=True, # 문장의 앞에 문장 시작을 알리는 \"[CLS]\"토큰, 문장의 끝에 문장 끝을 알리는 \"[SPE]\"토큰을 자동으로 추가\n","    return_tensors='pt' # pytorch tensor로 반환할 것\n",")\n","print(f\"Tokenized Sentence: {tensor_sent}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"rtXP_wRFLnJ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136517413,"user_tz":-540,"elapsed":118,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"f15fa3f0-8631-4378-c2de-64c28862a938"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence 1: 호주판 로빈훗. 소름끼치도록 진실을 왜곡하다 .\n","Original Sentence 2: ㅈㄹ하지말고 21일날 개봉이라매 도대체 어디서 개봉한건디?롯데시네마라고 해서 홈피 가봤는데 개봉예정에 떴다가 21일 되니까 아무정보도 없네 이런 디질랜드 보고싶다고\n","\n","Tokenized Sentence list: {'input_ids': tensor([[    2,  6248,  2025, 18512,  3519,    18, 16109,  2274,  2225, 16818,\n","          5352,  2069,  7399,  2205,  2062,    18,     3,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0],\n","        [    2,   190,  2317,  2205,  2118,  2231,  2088,  4041,  2210,  2401,\n","          6837,  2052,  2181,  2077,  6641,  4069,  2112,  6837,  2470,  2332,\n","          2220,    35, 27800,  7245,  3689, 26152,   543,  3072, 13964,  6837,\n","          2194,  2287,  2170,   914,  4795,  4041,  2210,   859,  3707,  3795,\n","         19861,  2119,  1415,  2203,  3667,   887, 12258,  4530,  2585,  4683,\n","             3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]])}\n"]}],"source":["# 토크나이징 예시 (2개의 문장)\n","\n","# 1. 2개의 문장을 가진 list 생성\n","ex_sent_list = list(sample_df.document.iloc[:2].values)\n","for i, sent in enumerate(ex_sent_list):\n","    print(f\"Original Sentence {i+1}: {sent}\")\n","\n","# 2. 문장 리스트를 토크나이즈 함. 이 때, 리스트 내 문장들의 토큰 길이가 동일할 수 있도록 가장 긴 문장을 기준으로 부족한 위치에 \"[PAD]\" 토큰을 추가\n","tensor_sent_list = tokenizer_bert(\n","    ex_sent_list,\n","    add_special_tokens=True,\n","    return_tensors='pt',\n","    padding=\"longest\" # 가장 긴 문장을 기준으로 token개수를 맞춤. 모자란 토큰 위치는 \"[PAD]\" 토큰을 추가\n",")\n","\n","print(f\"\\nTokenized Sentence list: {tensor_sent_list}\")\n","\n","# 토크나이즈 된 두 문장의 길이가 동일함을 검증\n","assert tensor_sent_list['input_ids'][0].shape == tensor_sent_list['input_ids'][1].shape "]},{"cell_type":"code","execution_count":42,"metadata":{"id":"tR22xs-xf1QH","executionInfo":{"status":"ok","timestamp":1646136988337,"user_tz":-540,"elapsed":6,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["def data_iterator(df, input_column, target_column, batch_size):\n","  \"\"\"\n","  데이터 프레임을 셔플한 후 \n","  데이터 프레임의 input_column을 batch_size만큼 잘라 토크나이즈 + 텐서화하고, target_column을 batch_size만큼 잘라 텐서화 하여\n","  (input, output) 튜플 형태의 이터레이터를 생성\n","  \"\"\"\n","\n","  global tokenizer_bert\n","\n","  # 1. 데이터 프레임 셔플\n","  #    pandas의 sample 함수 사용\n","  df = df.sample(frac=1) # 1의 비율로 랜덤샘플링하는 방식으로 shuffle\n","\n","  # 2. 이터레이터 생성\n","  for idx in range(0, df.shape[0], batch_size):\n","    batch_df = df[:batch_size]\n","    \n","    tensorized_input = tokenizer_bert(\n","        str(batch_df[[input_column]]), \n","        add_special_tokens=True, \n","        return_tensors='pt', \n","        padding=\"longest\") # df의 text를 토크나이징 + token id로 변환 + 텐서화 (df의 input_column 사용)\n","    \n","    tensorized_target = df[target_column] # target(label)을 텐서화 (df의 target_column 사용)\n","    \n","    yield tensorized_input, tensorized_target # 튜플 형태로 yield"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"zTlAV0hqILmc","executionInfo":{"status":"ok","timestamp":1646136988339,"user_tz":-540,"elapsed":7,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["batch_size=32\n","train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"P9VNAMchf1QI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136988795,"user_tz":-540,"elapsed":4,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"da8e2402-983f-4c13-f192-cbaade9b1bbf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'input_ids': tensor([[    2, 21287,  2046,  6306,  5248,  5732, 24232,  2083,  7027,  3771,\n","           7245,    18,    18,    18,  6448, 24803,  2302,   732,  2116,  2136,\n","          16516,  2284,  7966,  2094,  2517,  2062,  2250,  4530,  2585,  2118,\n","           2872,  2062,  4232, 24075,  2248,  7478,  2053, 12413,  3667,  2422,\n","          16460,  2792,  2075,  2907,  6076,    18,    18,    18, 31160, 25302,\n","           1504,  3771,   692,  3468,  3725,  6163,  2757,  2227,  2112,  9715,\n","          19521,  2914,    18,    18,    18,  5935, 20835,  2050,   721,  2560,\n","           2051,  2181,    97,  1164,  2154,  2205,  2062,  5842, 22400,  2302,\n","           1376,     5,  5861,  2052,  2173,    97,  4346,  9616,  2082,   653,\n","           2299,    97,  6586, 21455,  2050, 16109,  2274,  2225, 16818,  5537,\n","           2470,  4665,  9922,  2409, 13079,  2088,  1597,  2371,  2219,  3606,\n","             18,  6437,  5267,  4761,  4380,  2391,  2529,  3725,  1552,  2741,\n","           2530,  2079, 25992,    18,  5076,  2170,  6992,  2414,  1268,  2664,\n","           2079, 16499,  2470,  8765,  2119,   780,  2170,  1378,  2031,    18,\n","             18,    18, 14018,  2082, 24179, 17535,  2047,  2119,  4245,  2470,\n","           2147,  4415, 31302,  3610,  1889,  6047, 16040, 26421,  2302, 20609,\n","           4221, 16178,  2139,  2532,  4221, 16178, 29846, 24426,  5349,  2097,\n","           2182,    18,  3893,  1237,  2073,  3611,    18, 14468, 23406,  2082,\n","          19688, 11765, 13160,    18, 17278,  7371,  2227,  2170,  4062,   543,\n","           2088,  1335, 10283,    97, 15727, 19613,  2082, 18104,  8169,  2116,\n","           5124,     1,    18,    18,    18,  6208,  2052, 10020,  7321,  2147,\n","          11878,  2205,  2118,  1380,  2073,  3771,  2116,   859,    18,    18,\n","             18,  8049, 24148,  2248, 27848,  2178,  2062, 23974,    18,    18,\n","             18,  6437, 22928,  1097,  2052,  2957,  4165,  2919,  3771,  4229,\n","           1898,  2406,  3468,  7215,  8860,  2049, 28467,  2207,  2520,  2420,\n","           2614,  1164,  2318,  1415,  2053,    18,  4165,  2116, 15021,  2062,\n","           7115,  5267,  2049, 14321,  2461,  2051,  7480, 20609,  4705,  2088,\n","           4661,  7604,  2470,   558,  2185,  2185,  6614, 16516,    18,  4165,\n","             16,  9657,  2227,  2299,  2667, 16771, 27252,  2083,    22,  2200,\n","           6145,  2275,  2371,  2062,    18,  5679, 24818,  2179,  2073,  1460,\n","          24135,  2168,  2180,  2275,    35,  1469,  2483,    97,  3956, 24395,\n","           2248,  3841,  2079,  8347,  9296,  7815, 25939,  2195,  3919,  1378,\n","           3072,  2062, 15727, 23577,  2236, 19875, 12190,   677,  2178,  5971,\n","              5,     5,     5,  6677, 21946,  2236,  7478,  2180,  2917,  3072,\n","          10283,  1389,  2893,  2677,  5147,  2689,  2203,  2182,  7725, 21717,\n","           1043,   773,  2088,  6001,  2318,  1164,  1295,  1513,  2259,  3771,\n","             18, 10522,  7556,  2302, 11103, 11800,    18,  4530,  2075,  2460,\n","              1,    18,  3696,  5261,  3660,  3616,  6894, 27135,  4652,  2119,\n","           3658,  2205,  2118,  2872,    18,    18,    18,  6550, 25079,  6573,\n","           2119,  2460, 17168,  4307,  2075,  2259, 19445,  2179,  2147, 20609,\n","           2052,   728,  2203,    18,  4178,  2052,  1556,   642,  2057,  4957,\n","           2496,  2307, 30772,    18,    18,    18,  5552, 25986,  2050,  1469,\n","           2220,  2079,  4717,  2333, 14018, 22083,  2049,   732,  2116,  1163,\n","            578, 10993,  3926, 18395,   602,  2215,  2015,  2328,  2104,  2116,\n","           9887,  5124,  3656,  3967,  2052,  7245,  3978,   861,  2471, 30923,\n","          23525,  3817,  3666,  2024,  2580,  2079,  3654,  2138, 10532, 11187,\n","           4074,  5643,  2112,  2119,  4557,  2138,  1041,  2205,  2259,    18,\n","             18,    18,  3771,  2155,  2093,  2119,  5825,  2530,  7247,  7556,\n","           2254,  7962,  2097,  2165,  2052, 16368,  2470,  8982,  2259,   575,\n","           2052,  1110,  2368,  2118,  2118,  1380,  2259,  3771,    18,    18,\n","           4017,  2434,  2755,  2755, 14814, 25986,  2302,  1504,  3771,  4229,\n","           1041,  2052,  1378,  2897,  2062,    18,  7948, 21717,  2195,  5420,\n","           2052,  4178,  2124,  2181,  2209, 13079,  2062,  3699,  4665,  1097,\n","           2178,  2406,  6516,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1]])}, 46981     0\n"," 66862     0\n"," 28786     1\n"," 74051     1\n"," 51174     1\n","          ..\n"," 15571     1\n"," 66328     1\n"," 79744     1\n"," 27806     1\n"," 130989    1\n"," Name: label, Length: 1000, dtype: int64)"]},"metadata":{},"execution_count":44}],"source":["next(train_iterator)"]},{"cell_type":"markdown","metadata":{"id":"Cqnp2Q6ZLnJ2"},"source":["## Advanced"]},{"cell_type":"markdown","metadata":{"id":"cQVTqAUxLnJ2"},"source":["### `data_iterator` 함수로 생성한 이터레이터를 for loop 돌면서 배치 단위의 데이터를 모델에 학습하는 `train()` 함수 구현\n","- 함수 정의\n","  - 입력 매개변수\n","    - `model` : BERT + 1 hidden layer classifier 모델\n","    - `data_iterator` : train data iterator\n","- Reference\n","  - [Loss: CrossEntropyLoss official document](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n","  - [Optimizer: AdamW official document](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"-sE7xjYcRD1p","executionInfo":{"status":"ok","timestamp":1646136517942,"user_tz":-540,"elapsed":571,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["from torch.optim import AdamW\n","from torch.nn import CrossEntropyLoss\n","from numpy.core.fromnumeric import nonzero"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"7Er1qKtsf1QJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646136680202,"user_tz":-540,"elapsed":2791,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"8602d1d9-17b6-414e-aeea-e4409191f07a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# 모델 클래스 정의\n","model = CustomClassifier(hidden_size=768, n_label=2)\n","\n","batch_size = 32\n","\n","# 데이터 이터레이터 정의 \n","train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)\n","\n","# 로스 및 옵티마이저\n","loss_fct = CrossEntropyLoss()\n","optimizer = AdamW(\n","    model.parameters(),\n","    lr=2e-5,\n","    eps=1e-8\n",")\n"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"ZvY5rxDKHQAp","executionInfo":{"status":"ok","timestamp":1646137225585,"user_tz":-540,"elapsed":429,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}}},"outputs":[],"source":["def train(model, data_iterator):\n","\n","  global loss_fct # 위에서 정의한 loss 함수\n","\n","  # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n","  total_loss, batch_loss, batch_count = 0,0,0\n","  \n","  # model을 train 모드로 설정 & device 할당\n","  model.train()\n","  model.to(device)\n","  \n","  # data iterator를 돌면서 하나씩 학습\n","  for step, batch in enumerate(data_iterator):\n","    batch_count+=1\n","    \n","    # tensor 연산 전, 각 tensor에 device 할당\n","    batch = tuple(item.to(device) for item in batch)\n","    \n","    batch_input, batch_label = batch\n","    \n","    # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n","    model.zero_grad()\n","    \n","    # forward\n","    logits = model(batch_input)\n","    \n","    # loss\n","    loss = loss_fct(logits, batch_label)\n","    batch_loss += loss.item()\n","    total_loss += loss.item()\n","    \n","    # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n","    loss.backward()\n","    \n","    # optimizer 업데이트\n","    optimizer.step()\n","      \n","    # 배치 10개씩 처리할 때마다 평균 loss를 출력\n","    if (step % 10 == 0 and step != 0):\n","      print(f\"Step : {step}, Avg Loss : {batch_loss / batch_count:.4f}\")\n","      \n","      # 변수 초기화 \n","      batch_loss, batch_count = 0,0\n","  \n","  print(f\"Mean Loss : {total_loss/(step+1):.4f}\")\n","  print(\"Train Finished\")"]},{"cell_type":"markdown","metadata":{"id":"OEYo8z9RLnJ2"},"source":["### 지금까지 구현한 함수와 클래스를 모두 불러와 `train()` 함수를 실행하자\n","- fine-tuning 모델 클래스 (`CustomClassifier`)\n","    - hidden_size = 768\n","    - n_label = 2\n","- 데이터 이터레이터 함수 (`data_iterator`)\n","    - batch_size = 32\n","- loss \n","    - `CrossEntropyLoss()`\n","- optimizer\n","    - optimizer는 loss(오차)를 상쇄하기 위해 파라미터를 업데이트 하는 과정\n","    - `optimizer.step()` 시 파라미터가 업데이트 됨 \n","    - lr = 2e-5\n","- Reference\n","  - [Optimizer 종류 설명 한국어 블로그 ](https://ganghee-lee.tistory.com/24)\n","    "]},{"cell_type":"code","execution_count":50,"metadata":{"id":"uCODIxCfFEDP","colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"status":"error","timestamp":1646137230062,"user_tz":-540,"elapsed":7,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"dce6b0a4-f627-4be3-80ce-d2cb6128181d"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-c4c5b9fef899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-49-e0581ad44abf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_iterator)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# tensor 연산 전, 각 tensor에 device 할당\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-49-e0581ad44abf>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# tensor 연산 전, 각 tensor에 device 할당\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'to'"]}],"source":["# 학습 시작\n","train(model, train_iterator)"]},{"cell_type":"markdown","metadata":{"id":"37UbAkh7LnJ3"},"source":["## fine-tuning 2가지 방법론 비교\n","- pre-trained BERT 모델 파라미터를 **freeze**한 채 학습하라\n","    - BERT의 파라미터의 `requires_grad` 값을 `False`로 바꾸면, 학습 시 BERT의 파라미터는 미분이 계산되지도, 업데이트 되지도 않는다. \n","    - 이렇게 특정 모델의 파라미터가 업데이트 하지 못하도록 설정하는 것을 **freeze**라고 한다. \n","    - BERT 파라미터를 freeze시킨 채 학습을 진행해보자. 이럴 경우, 우리가 직접 쌓은 fine-tuning layer의 파라미터만 업데이트 된다. \n","- **unfreeze**와 **freeze** 모델의 성능을 비교해 보자. 어떤 방식이 더 우수한가?\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ld260K3YLnJ3"},"outputs":[],"source":["class CustomClassifierFreezed(nn.Module):\n","\n","  def __init__(self, hidden_size: int, n_label: int):\n","    super(CustomClassifierFreezed, self).__init__()\n","\n","    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n","    # freeze BERT parameter\n","    # BERT의 파라미터는 고정값으로 두고 BERT 위에 씌운 linear layer의 파라미터만 학습하려고 한다. \n","    # 이 경우, BERT의 파라미터의 'requires_grad' 값을 False로 변경해줘야 학습 시 해당 파라미터의 미분값이 계산되지 않는다.\n","    for param in self.bert.parameters():\n","        None\n","\n","    dropout_rate = 0.1\n","    linear_layer_hidden_size = 32\n","\n","    self.classifier = nn.Sequential(None)\n","\n","      \n","  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","\n","    outputs = self.bert(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","    )\n","    \n","    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n","    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n","    logits = self.classifier(cls_token_last_hidden_states)\n","\n","    return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ClEEHB6F6LW"},"outputs":[],"source":["# freeze 모델\n","# model을 제외한 설정값은 \b위에서 실행한 unfreeze 모델과 동일\n","model = None\n","\n","# 데이터 이터레이터\n","batch_size = None \n","train_iterator = None\n","\n","# 로스 및 옵티마이저\n","loss_fct = None\n","optimizer = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqcPcWZeLnJ3"},"outputs":[],"source":["# 학습 시작\n","train(model, train_iterator)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"김예신 - Week2_2_assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3acfaa50c5304093b8e00203c6801a2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ab269b1881f4ab3b489469bd1245389","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8076dae6044a45da92723900cda75ac8","IPY_MODEL_1a4ef91de8744ff188dd6e30e9caebff","IPY_MODEL_cf5d77276a9f42aa8eb135b446020af3"]}},"7ab269b1881f4ab3b489469bd1245389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8076dae6044a45da92723900cda75ac8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cf4635178eee419d9d95aaf12b870379","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5eaea918576a45fb8c7bcc284ec06be2"}},"1a4ef91de8744ff188dd6e30e9caebff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4630aabfb7ca4257ab72ed623278c687","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":289,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":289,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fae16c504b02402391b7ffcd554aab89"}},"cf5d77276a9f42aa8eb135b446020af3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b50d556c91ed4ba684125bb285227fcf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 289/289 [00:00&lt;00:00, 6.82kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4d04d42212664ea08123952e207534b3"}},"cf4635178eee419d9d95aaf12b870379":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5eaea918576a45fb8c7bcc284ec06be2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4630aabfb7ca4257ab72ed623278c687":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fae16c504b02402391b7ffcd554aab89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b50d556c91ed4ba684125bb285227fcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4d04d42212664ea08123952e207534b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb89304cecf74e86ac220cb82fcea8d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cc0dc80af7ab461da24ae4baf24820ae","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_89035877e6ea4a8c875b6ce3cd25b0da","IPY_MODEL_4c7fd8263cba4634bb10276c71d7cef7","IPY_MODEL_3f0d524e62c044d8b0d4bd68e4e7ca2f"]}},"cc0dc80af7ab461da24ae4baf24820ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89035877e6ea4a8c875b6ce3cd25b0da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_67ce9c64c331433eb31cb8bb55568375","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28b887641f624ad0b0e9410b1aafd53c"}},"4c7fd8263cba4634bb10276c71d7cef7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_84fe90e99f3c437a9100795948228fa4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":248477,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":248477,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_07c15b3a87794cfa8188e318099c1c3e"}},"3f0d524e62c044d8b0d4bd68e4e7ca2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6c527225f7154eed86d647715c7d17a6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 243k/243k [00:00&lt;00:00, 657kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2621d38fad7d4169aad9d3b64dda54d1"}},"67ce9c64c331433eb31cb8bb55568375":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"28b887641f624ad0b0e9410b1aafd53c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84fe90e99f3c437a9100795948228fa4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"07c15b3a87794cfa8188e318099c1c3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c527225f7154eed86d647715c7d17a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2621d38fad7d4169aad9d3b64dda54d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d93a37fab7614aa383f17c15a7e9505d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0d42abb7209d4c7580964f079867f3cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_13571ef00ec34d078a1a29255d957be2","IPY_MODEL_fe4fdabb9fc24384a080e17bbf5c4b99","IPY_MODEL_05c08389cab0466e924690c3c52ca137"]}},"0d42abb7209d4c7580964f079867f3cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"13571ef00ec34d078a1a29255d957be2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_26825ab0ac884175adbe12b04eb7ec0b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f42b75d58d494247803eca3d9dec5372"}},"fe4fdabb9fc24384a080e17bbf5c4b99":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_85d3c2b434064f72af7cb335e2173529","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":125,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":125,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8c5cd6a3b4604d5e87328c32b210ac22"}},"05c08389cab0466e924690c3c52ca137":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ce661b2529c247bbb0ffe147791b24de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 125/125 [00:00&lt;00:00, 2.29kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b27da44ed76495e95dcf4d0e7366eeb"}},"26825ab0ac884175adbe12b04eb7ec0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f42b75d58d494247803eca3d9dec5372":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85d3c2b434064f72af7cb335e2173529":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8c5cd6a3b4604d5e87328c32b210ac22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce661b2529c247bbb0ffe147791b24de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2b27da44ed76495e95dcf4d0e7366eeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49a1001e923b4f1d83b0a0a4d03c3821":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7c72ad75371242a8b20dc0257e7e02e8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c0702b08dd1d483f99b05bcc11280cbc","IPY_MODEL_70213bd876234ba197d2376d35abea7b","IPY_MODEL_de1c25e1d6e44148900768cb1921fce3"]}},"7c72ad75371242a8b20dc0257e7e02e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c0702b08dd1d483f99b05bcc11280cbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_353fdbe6224c479bbfc64bbefbb762c7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_67f40f9ec5a64e2f83c75d0c0542ccee"}},"70213bd876234ba197d2376d35abea7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_98f020424e1b4e888e866571e0505b91","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":494860,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":494860,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29a89d9fb1e7437fb7981466930b89bc"}},"de1c25e1d6e44148900768cb1921fce3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a139f2da1f34585927968051eb734d7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 483k/483k [00:00&lt;00:00, 864kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_856f0caf33164b1295014de5036f0473"}},"353fdbe6224c479bbfc64bbefbb762c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"67f40f9ec5a64e2f83c75d0c0542ccee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"98f020424e1b4e888e866571e0505b91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"29a89d9fb1e7437fb7981466930b89bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a139f2da1f34585927968051eb734d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"856f0caf33164b1295014de5036f0473":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a5aa6b35f8d84c3aa8ecac05bd179490":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_acedf58755e84ae791a8924ff12748a3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6fcf3a85d02d412a8c4cb4d1fe24e1ea","IPY_MODEL_eb6bd0883c56423880a3562e64d0ce3d","IPY_MODEL_e925804e8cf1473cafa1b34aa684ff91"]}},"acedf58755e84ae791a8924ff12748a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6fcf3a85d02d412a8c4cb4d1fe24e1ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b8fc755368ac4054aabb2fddb15789c3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f7035ebf2684efda505144482e84c7e"}},"eb6bd0883c56423880a3562e64d0ce3d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1396a7f597d94bb7b3776614e13bc463","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":425,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":425,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8a2ed76f5d045f38fbdcefd379efd1b"}},"e925804e8cf1473cafa1b34aa684ff91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fd43ecd0a6e9452c8b51dd042f17c0aa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 425/425 [00:00&lt;00:00, 6.87kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a55ea57425e445af98bd4c807825d6bd"}},"b8fc755368ac4054aabb2fddb15789c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f7035ebf2684efda505144482e84c7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1396a7f597d94bb7b3776614e13bc463":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b8a2ed76f5d045f38fbdcefd379efd1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd43ecd0a6e9452c8b51dd042f17c0aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a55ea57425e445af98bd4c807825d6bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}