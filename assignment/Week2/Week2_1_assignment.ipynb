{"cells":[{"cell_type":"markdown","metadata":{"id":"Hg3yVV-zjev_"},"source":["# Week2_1 Assignment\n","\n","# [BASIC](#Basic)\n","- BERT 모델의 hidden state에서 **특정 단어의 embedding을 여러 방식으로 추출 및 생성**할 수 있다.\n","\n","# [CHALLENGE](#Challenge)\n","- **cosine similarity 함수를 구현**할 수 있다. \n","- **단어들의 유사도**를 cosine similarity로 비교할 수 있다. \n","\n","# [ADVANCED](#Advanced)\n","- 문장 embedding을 구해 **문장 간 유사도**를 구할 수 있다.\n","\n","### Reference\n","- [BERT word embedding & sentence embedding tutorial 영문 블로그](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#33-creating-word-and-sentence-vectors-from-hidden-states)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"audBCE5fjSMC"},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","import torch\n","import random"]},{"cell_type":"markdown","metadata":{"id":"T03wL1uH1HHb"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"cvJncy-fjkUV"},"source":["### BERT 모델과 토크나이저 로드   \n","- 두 사람의 대화에서 (단어 및 문장의) embedding을 생성하고자 한다. 아래 대화를 BERT 모델에 입력해 출력값 중 \"hidden states\"값을 가져오자.\n","- `Hidden States`는 3차원 텐서를 가지고 있는 list 타입이다. List에는 BERT 모델의 각 layer마다의 hidden state 3차원 텐서를 갖고 있으며 각 텐서는 (batch_size, sequence_length, hidden_size) shape을 가진다. BERT-base 모델은 12 layer를 갖고 있고 이와 별도로 Embedding Layer 1개를 더 갖고 있기 때문에 `len(hidden states)`는 13개가 된다. \n","    - batch_size: 학습 시 설정한 배치 사이즈. 또는 BERT 모델에 입력된 문장의 개수\n","    - sequence_length: 문장의 token의 개수. \n","    - hidden size: token의 embedding size \n","- Reference\n","    - [BertTokenizer.tokenize() 함수의 매개변수 설명](https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__)\n","    - [BERTModel.forward() 함수의 매개변수 및 리턴 값 설명](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcmmeNkujk0x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647162755815,"user_tz":-540,"elapsed":13491,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"01d009f6-5568-4a70-cc6e-ef826842f5e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 9.1 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 50.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 51.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["# !pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZqEFuRHjgBj"},"outputs":[],"source":["from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXyCD5dnjo37","colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["f28af55b36874e22b2fe19713e2ffa9b","aabef576559c4b52ba6c1d2747d2897d","8c67d91ce5324dfabb826c57d92bd198","6c1791149d3c4268ad3a2e7fe2295cb3","2f76e8e951e34db2a7a786252f7bd18c","918ceae593ad45cfaec9b090ac7bc284","e0ca89d48b87413fb4a72f41fd042983","00759d0c074c467ba620077746fb12a2","62ff82de4df540e4a1676b3128a77aea","86847caf313142bdad969335d7e9769e","460769fec83e4aaeb9d4e5a51f7f5068","cb881965fb7e462c905c65cb01d4757d","72bae30c20264d19a87f5d54a45068ab","adfbd232150e49479c0577eebc3b1c76","ca6a83d3c2f94453b7dc91c98215f31d","f128e2b76ef14deab246acc910520131","02ed1a6e15004f158e55379814f4a8d2","8edbc4e974404e6ca1310e626548a7b7","38a665ab7a6140b2b2bcff708a52980e","3891068161c749e18b62115fd8a86c88","5d1540962a50420f89687ac6644c791a","e947aa426fcc4f518bf1c91dd7214641","c52f685d717549d18f15a8e3c28e5c6b","0bee48ea2c9e42bb874733e6133e49aa","cb7f9526816f45bbba88afde3d863fc2","fa767f5ab48e41d7a0c092c582f2846a","c684a6f8fd2c4ec89ced98be7b9d3b0e","790cacb19dd84286a978504b6a52ba03","e04b01bc43f54758967821f4f235f8ba","bbc448972f98470885eb7f11c0c1d0a0","b4d383e335f346e6831da139824df646","6ebfbeee6f27498092ed63d0e2109eaa","b3f2ad05a28c4419a94b5ec98051dcb6","920f60abcba64b8d83250e024d79bcf4","aeb7632c0f264594b1995284aa235cbb","7248e22ed0ea4f3980363f2537b33658","ed6ca558df184e01921fe80964fb3413","0aba88ae95e240dca2d62a2b5233c157","b0001045beeb440ba0af069c0a4695a6","06ba269fdc1c4c2dae7323b50d285baa","ac2812c510e14b9a957ec09f65fad476","dd0f338df0704791a4077da74bae28f5","97334c279cf54e2ab80c2dc032502eff","55dba1218dc644d999e4ea781f95fcac"]},"executionInfo":{"status":"ok","timestamp":1647162787188,"user_tz":-540,"elapsed":16698,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"a9228686-fb81-4e56-c1fd-6fab4a281880"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f28af55b36874e22b2fe19713e2ffa9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb881965fb7e462c905c65cb01d4757d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52f685d717549d18f15a8e3c28e5c6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"920f60abcba64b8d83250e024d79bcf4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","model_bert = BertModel.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-UbcLH4juKA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647162792608,"user_tz":-540,"elapsed":396,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"50c158e0-28b1-42e6-8379-1e90b9641b46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Normal Person asked: what do you do when you have free time?\n","Nerd answers: I code. code frees my minds, body and soul.\n","Normal Person asked: (what a nerd...) coding?\n","Nerd answers: Yes. coding is the best thing to do in the free time.\n"]}],"source":["normal_person = [\"what do you do when you have free time?\"]\n","nerd = [\"I code. code frees my minds, body and soul.\"]\n","normal_person.append(\"(what a nerd...) coding?\")\n","nerd.append(\"Yes. coding is the best thing to do in the free time.\")\n","\n","for i in range(len(normal_person)):\n","    print(f\"Normal Person asked: {normal_person[i]}\")\n","    print(f\"Nerd answers: {nerd[i]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQtKFM-hjvVl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647162856358,"user_tz":-540,"elapsed":498,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"3046a0e1-d0cc-4578-af43-5d06f9d9ee2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 28])\n"]}],"source":["# 매개변수 설명\n","# truncation <- max_len 넘어가지 않도록 자르기\n","# padding <- max(seq_len, max_len) zero padding\n","# return_tensors <- return 2d tensor \n","\n","inputs = tokenizer_bert(\n","    text = normal_person,\n","    text_pair = nerd,\n","    truncation = True,\n","    padding = \"longest\", \n","    return_tensors='pt'\n","    )\n","\n","print(inputs['input_ids'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjua8EtijyJs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647162869710,"user_tz":-540,"elapsed":3038,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"c39d5b03-215e-4d82-e427-3317e4703c14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Coversation 0 -> '[CLS] what do you do when you have free time? [SEP] I code. code frees my minds, body and soul. [SEP] [PAD] [PAD]'\n","Coversation 1 -> '[CLS] ( what a nerd... ) coding? [SEP] Yes. coding is the best thing to do in the free time. [SEP]'\n"]}],"source":["# decoding\n","for i in range(len(inputs['input_ids'])):\n","    print(f\"Coversation {i} -> '{tokenizer_bert.decode(inputs['input_ids'][i])}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggtYUkbejzBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647162869710,"user_tz":-540,"elapsed":6,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"4ddf191c-d2d3-47c9-811d-73f85f701081"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3463]"]},"metadata":{},"execution_count":7}],"source":["# \"code\" 단어의 token id(각 단어에게 고유하게 주어진 id)를 출력\n","tokenizer_bert.encode('code', add_special_tokens=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZUxJJxkj0NG","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"error","timestamp":1647162878220,"user_tz":-540,"elapsed":415,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"ccb46d0c-4a4d-4829-cfbc-66db61ca577f"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-2e5e4c899c5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5TrsQ_Wj09O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646033515666,"user_tz":-540,"elapsed":324,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"b80b2e97-a1c5-4cfe-d305-bf435ead6e5a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":11}],"source":["# 입력 데이터와 BERT 모델을 \"GPU\" 장치로 로드함\n","inputs = inputs.to(device)\n","model_bert.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FGm_9rtj1xw"},"outputs":[],"source":["# 입력 데이터를 BERT 모델에 넣어 출력값을 가져옴\n","outputs = model_bert(\n","    **inputs, \n","    output_hidden_states=True\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9jsHuLjj3Sy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646033527172,"user_tz":-540,"elapsed":378,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"f0ff3ba1-13a1-4a44-80cc-517e2cf310b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"]},"metadata":{},"execution_count":13}],"source":["outputs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0nB31XAj4M_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646033530898,"user_tz":-540,"elapsed":326,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"7836b774-fffc-4baf-f1ef-99b0c678d3ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["# layers : 13\n","tensor shape in each layer : torch.Size([2, 28, 768])\n"]}],"source":["hidden_states = outputs['hidden_states']\n","print(f\"# layers : {len(hidden_states)}\")\n","print(f\"tensor shape in each layer : {hidden_states[-1].shape}\")"]},{"cell_type":"markdown","metadata":{"id":"mfy3I4FXj6L_"},"source":["###  Q1. 1번째 sequence (문장)에서 \"code\"라는 단어의 인덱스를 모두 반환하라.\n","- \"code\" 단어는 총 2개 존재 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tYG4o_ho1QY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646036315124,"user_tz":-540,"elapsed":329,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"6b68ae3a-664f-434c-dc8b-b245ba1e9872"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[13],\n","        [15]])\n"]}],"source":["def get_index(seq, word):\n","  embed = tokenizer_bert.encode(word, add_special_tokens=False)[0]\n","  index = (seq == embed).nonzero()\n","  return index\n","\n","# input\n","# seq1: 1번째 sequence\n","# token: 단어\n","seq1 = inputs['input_ids'][0]\n","token = \"code\"\n","\n","# output\n","token_index = get_index(seq1, token)\n","print(token_index)"]},{"cell_type":"markdown","metadata":{"id":"xopkXDS2m6uA"},"source":["### Q2. 1번째 sequence의 1번째 \"code\" 토큰의 embedding을 여러가지 방식으로 구하고자 한다. BERT hidden state를 다음의 방식으로 인덱싱해 embedding을 구하라\n","- 1 layer\n","- last layer\n","- sum all 12 layers\n","- sum last 4 layers\n","- concat last 4 layers\n","- average last 4 layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"to7IOGNwkmUS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646043456125,"user_tz":-540,"elapsed":323,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"04b8ae48-aa63-4891-ae03-cc96124da577"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 28, 768])\n","torch.Size([2, 28, 768])\n","torch.Size([13, 2, 28, 768])\n","torch.Size([2, 28, 768])\n","torch.Size([8, 28, 768])\n","torch.Size([2, 28, 768])\n"]}],"source":["# 1 layer\n","first_layer_emb = hidden_states[1][]\n","print(first_layer_emb.shape)\n","\n","# last layer\n","last_layer_emb = hidden_states[-1]\n","print(last_layer_emb.shape)\n","\n","# sum all 12 layers\n","sum_all_layer_emb = torch.stack(hidden_states, dim=0)\n","print(sum_all_layer_emb.shape)\n","\n","# sum last 4 layers\n","sum_last4_layer_emb = sum(hidden_states[-4:]).squeeze()\n","print(sum_last4_layer_emb.shape)\n","\n","# concat last 4 layers\n","concat_last4_layer_emb = torch.cat(hidden_states[-4:])\n","print(concat_last4_layer_emb.shape)\n","\n","# mean last 4 layers\n","mean_last4_layer_emb = sum_last4_layer_emb/len(hidden_states)\n","print(mean_last4_layer_emb.shape)"]},{"cell_type":"code","source":[""],"metadata":{"id":"HpDJHSDw7F3r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i0-C0J6o1HHl"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"H3aI-M5KubYl"},"source":["### Q3. `sum_last_four_layer` 방식으로 1번째 sequence의 2개의 \"code\" 토큰 사이의 코사인 유사도를 계산하라"]},{"cell_type":"code","source":["from scipy.spatial.distance import cosine"],"metadata":{"id":"-5NpKgQO1CSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twWm2hrrp3qG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646048065011,"user_tz":-540,"elapsed":331,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"8175c13f-f8cb-469c-ae6c-777b3a394297"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.5982182621955872\n"]}],"source":["def cosine_similarity_manual(x, y, small_number=1e-8):\n","  result = 1 - cosine(x, y)\n","\n","  return result\n","\n","# input\n","# x: 1번째 sequence의 1번째 \"code\"의 sum_last_four_layer 방식 embedding\n","# y: 1번째 sequence의 2번째 \"code\"의 sum_last_four_layer 방식 embedding\n","x = sum_last4_layer_emb[0][token_index][0].detach().numpy()\n","y = sum_last4_layer_emb[0][token_index][1].detach().numpy()\n","\n","# output\n","score = cosine_similarity_manual(x, y)\n","print(score)"]},{"cell_type":"markdown","metadata":{"id":"Wfc_1t2Hw8kB"},"source":["### Q4. 2번째 sequence에서 \"coding\"이라는 토큰의 위치를 반환하라"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_OrrpEgw9pX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646048069609,"user_tz":-540,"elapsed":339,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"8c01f466-59bf-45b6-bf7f-fb4b985d9628"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[10],\n","        [15]])\n"]}],"source":["# Q1과 동일한 문제 \n","\n","# input\n","# seq1: 2번째 sequence\n","# token: 단어\n","seq2 = inputs['input_ids'][1]\n","token = \"coding\"\n","\n","# output\n","# Q1에서 구현한 함수 사용\n","token_index = get_index(seq2, token)\n","print(token_index)"]},{"cell_type":"markdown","metadata":{"id":"c11W2Ht-xIwI"},"source":["### Q5. `concat_last4_layer_emb` 방식으로 2번째 sequence의 2개의 \"coding\" 토큰 사이의 코사인 유사도를 계산하라"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"MXr1jtMOxKie","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"error","timestamp":1647188419212,"user_tz":-540,"elapsed":1232,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"0ffe0284-d002-4cb0-b140-8e1dc7f1a730"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ae528b98ef9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# x: 2번째 sequence의 1번째 \"coding\"의 concat_last4_layer_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# y: 2번째 sequence의 2번째 \"coding\"의 concat_last4_layer_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_last4_layer_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_last4_layer_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sum_last4_layer_emb' is not defined"]}],"source":["# Q3과 동일한 문제\n","\n","# input\n","# x: 2번째 sequence의 1번째 \"coding\"의 concat_last4_layer_emb\n","# y: 2번째 sequence의 2번째 \"coding\"의 concat_last4_layer_emb\n","x = sum_last4_layer_emb[1][token_index][0].detach().numpy()\n","y = sum_last4_layer_emb[1][token_index][1].detach().numpy()\n","\n","# output\n","# Q3에서 구현한 함수 사용\n","score = cosine_similarity_manual(x, y)\n","print(score)"]},{"cell_type":"markdown","metadata":{"id":"doBSxlvsxlZp"},"source":["### Q6. 2번째 sequence에서 랜덤하게 토큰 하나를 뽑아보자. 그 랜덤 토큰과 2번째 sequence의 2번째 \"coding\" 토큰의 코사인 유사도를 계산해보자"]},{"cell_type":"code","source":["import random"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W50vLS_vJ1XV","executionInfo":{"status":"ok","timestamp":1646051369658,"user_tz":-540,"elapsed":555,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"c93973c5-96c5-4d2b-d3dc-e5e01a6a8963"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28"]},"metadata":{},"execution_count":222}]},{"cell_type":"code","source":["int(random.choice(inputs['input_ids'][1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDiEklkxT9cy","executionInfo":{"status":"ok","timestamp":1646051571498,"user_tz":-540,"elapsed":355,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"1846cc4d-f2b2-49f7-f24a-ed8a526048d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19350"]},"metadata":{},"execution_count":235}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PajBEOs5xnOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646051592667,"user_tz":-540,"elapsed":16,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"38ce9995-88d8-4748-80b8-0aeb4a337f78"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.5653952956199646\n"]},{"output_type":"execute_result","data":{"text/plain":["1714"]},"metadata":{},"execution_count":238}],"source":["# input\n","# random_idx: random 모듈 사용하여 뽑은 랜덤 토큰의 인덱스\n","# random_word: random_idx에 해당하는 단어\n","# x: 2번째 sequence의 2번째 \"coding\" 토큰의 concat_last4_layer_emb\n","# y: 랜덤 토큰의 concat_last4_layer_emb\n","\n","random_idx = int(random.choice(inputs['input_ids'][1]))\n","random_word = tokenizer_bert.convert_ids_to_tokens(random_idx)\n","\n","token_index = get_index(inputs['input_ids'][1], random_word)\n","token_indey = get_index(inputs['input_ids'][1], 'coding')\n","\n","x = sum_last4_layer_emb[1][token_index][0].detach().numpy()\n","y = sum_last4_layer_emb[1][token_indey][1].detach().numpy()\n","\n","# output\n","# Q3에서 구현한 함수 사용\n","score = cosine_similarity_manual(x, y)\n","print(score)\n","random_idx"]},{"cell_type":"markdown","metadata":{"id":"hX9_cFhm1HHm"},"source":["## Advanced"]},{"cell_type":"markdown","metadata":{"id":"ub-hubXwyw9l"},"source":["### Q7. 1번째 sequence와 2번째 sequence의 문장 유사도를 구해보자. 문장의 엠베딩은 마지막 레이어의 첫번째 토큰 ('[CLS]')으로 생성한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpvPDM_Oyx7I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646051847546,"user_tz":-540,"elapsed":546,"user":{"displayName":"김예신","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16502918623196199690"}},"outputId":"e884e096-ca69-4554-df5b-3921ef90ce6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.2962106406276187\n"]}],"source":["# input\n","# x: 1번째 sequence의 embedding\n","# y: 2번째 sequence의 embedding\n","x = inputs['input_ids'][0]\n","y = inputs['input_ids'][1]\n","\n","# output\n","# Q3에서 구현한 함수 사용\n","score =  cosine_similarity_manual(x, y)\n","print(score)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"김예신 - Week2_1_assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f28af55b36874e22b2fe19713e2ffa9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aabef576559c4b52ba6c1d2747d2897d","IPY_MODEL_8c67d91ce5324dfabb826c57d92bd198","IPY_MODEL_6c1791149d3c4268ad3a2e7fe2295cb3"],"layout":"IPY_MODEL_2f76e8e951e34db2a7a786252f7bd18c"}},"aabef576559c4b52ba6c1d2747d2897d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_918ceae593ad45cfaec9b090ac7bc284","placeholder":"​","style":"IPY_MODEL_e0ca89d48b87413fb4a72f41fd042983","value":"Downloading: 100%"}},"8c67d91ce5324dfabb826c57d92bd198":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00759d0c074c467ba620077746fb12a2","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62ff82de4df540e4a1676b3128a77aea","value":213450}},"6c1791149d3c4268ad3a2e7fe2295cb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86847caf313142bdad969335d7e9769e","placeholder":"​","style":"IPY_MODEL_460769fec83e4aaeb9d4e5a51f7f5068","value":" 208k/208k [00:00&lt;00:00, 670kB/s]"}},"2f76e8e951e34db2a7a786252f7bd18c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"918ceae593ad45cfaec9b090ac7bc284":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ca89d48b87413fb4a72f41fd042983":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00759d0c074c467ba620077746fb12a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62ff82de4df540e4a1676b3128a77aea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86847caf313142bdad969335d7e9769e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"460769fec83e4aaeb9d4e5a51f7f5068":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb881965fb7e462c905c65cb01d4757d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72bae30c20264d19a87f5d54a45068ab","IPY_MODEL_adfbd232150e49479c0577eebc3b1c76","IPY_MODEL_ca6a83d3c2f94453b7dc91c98215f31d"],"layout":"IPY_MODEL_f128e2b76ef14deab246acc910520131"}},"72bae30c20264d19a87f5d54a45068ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ed1a6e15004f158e55379814f4a8d2","placeholder":"​","style":"IPY_MODEL_8edbc4e974404e6ca1310e626548a7b7","value":"Downloading: 100%"}},"adfbd232150e49479c0577eebc3b1c76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_38a665ab7a6140b2b2bcff708a52980e","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3891068161c749e18b62115fd8a86c88","value":29}},"ca6a83d3c2f94453b7dc91c98215f31d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d1540962a50420f89687ac6644c791a","placeholder":"​","style":"IPY_MODEL_e947aa426fcc4f518bf1c91dd7214641","value":" 29.0/29.0 [00:00&lt;00:00, 443B/s]"}},"f128e2b76ef14deab246acc910520131":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02ed1a6e15004f158e55379814f4a8d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8edbc4e974404e6ca1310e626548a7b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38a665ab7a6140b2b2bcff708a52980e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3891068161c749e18b62115fd8a86c88":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d1540962a50420f89687ac6644c791a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e947aa426fcc4f518bf1c91dd7214641":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c52f685d717549d18f15a8e3c28e5c6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bee48ea2c9e42bb874733e6133e49aa","IPY_MODEL_cb7f9526816f45bbba88afde3d863fc2","IPY_MODEL_fa767f5ab48e41d7a0c092c582f2846a"],"layout":"IPY_MODEL_c684a6f8fd2c4ec89ced98be7b9d3b0e"}},"0bee48ea2c9e42bb874733e6133e49aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_790cacb19dd84286a978504b6a52ba03","placeholder":"​","style":"IPY_MODEL_e04b01bc43f54758967821f4f235f8ba","value":"Downloading: 100%"}},"cb7f9526816f45bbba88afde3d863fc2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbc448972f98470885eb7f11c0c1d0a0","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4d383e335f346e6831da139824df646","value":570}},"fa767f5ab48e41d7a0c092c582f2846a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ebfbeee6f27498092ed63d0e2109eaa","placeholder":"​","style":"IPY_MODEL_b3f2ad05a28c4419a94b5ec98051dcb6","value":" 570/570 [00:00&lt;00:00, 11.5kB/s]"}},"c684a6f8fd2c4ec89ced98be7b9d3b0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"790cacb19dd84286a978504b6a52ba03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e04b01bc43f54758967821f4f235f8ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbc448972f98470885eb7f11c0c1d0a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4d383e335f346e6831da139824df646":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ebfbeee6f27498092ed63d0e2109eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3f2ad05a28c4419a94b5ec98051dcb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"920f60abcba64b8d83250e024d79bcf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aeb7632c0f264594b1995284aa235cbb","IPY_MODEL_7248e22ed0ea4f3980363f2537b33658","IPY_MODEL_ed6ca558df184e01921fe80964fb3413"],"layout":"IPY_MODEL_0aba88ae95e240dca2d62a2b5233c157"}},"aeb7632c0f264594b1995284aa235cbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0001045beeb440ba0af069c0a4695a6","placeholder":"​","style":"IPY_MODEL_06ba269fdc1c4c2dae7323b50d285baa","value":"Downloading: 100%"}},"7248e22ed0ea4f3980363f2537b33658":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac2812c510e14b9a957ec09f65fad476","max":435779157,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd0f338df0704791a4077da74bae28f5","value":435779157}},"ed6ca558df184e01921fe80964fb3413":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97334c279cf54e2ab80c2dc032502eff","placeholder":"​","style":"IPY_MODEL_55dba1218dc644d999e4ea781f95fcac","value":" 416M/416M [00:10&lt;00:00, 41.1MB/s]"}},"0aba88ae95e240dca2d62a2b5233c157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0001045beeb440ba0af069c0a4695a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06ba269fdc1c4c2dae7323b50d285baa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac2812c510e14b9a957ec09f65fad476":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd0f338df0704791a4077da74bae28f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"97334c279cf54e2ab80c2dc032502eff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55dba1218dc644d999e4ea781f95fcac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}