{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "원티드  프리온보딩 코스.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 1) Tokenizer 생성하기**\n"
      ],
      "metadata": {
        "id": "Im36vp9V8_2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규표현식\n",
        "import re"
      ],
      "metadata": {
        "id": "EiyfMxAjdZV0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    '''\n",
        "    문제 1-1.\n",
        "    '''\n",
        "    for i in sequences:\n",
        "      text = re.sub(\"[^ a-zA-Z0-9]\", '', i).lower().split(' ')\n",
        "      result.append(text)\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    '''\n",
        "    문제 1-2.\n",
        "    '''\n",
        "    token_list = self.preprocessing(sequences)\n",
        "    for tokens in token_list:\n",
        "      for token in tokens:\n",
        "        if token in self.word_dict:\n",
        "          self.word_dict[token] += 1\n",
        "        else:\n",
        "          self.word_dict[token] = 1\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "      '''\n",
        "      문제 1-3.\n",
        "      '''\n",
        "      token_list = self.preprocessing(sequences)\n",
        "      # indexing 위한 dict 생성\n",
        "      idict = {}\n",
        "      for idx, key in enumerate(self.word_dict):\n",
        "        idict[key] = idx\n",
        "\n",
        "      # comprehesion ver.\n",
        "      # idict = {key : idx for idx, key in enumerate(word_dict)}\n",
        "      \n",
        "      for tokens in token_list:\n",
        "        tmp = []\n",
        "        for token in tokens:\n",
        "          if token in self.word_dict:\n",
        "            tmp.append(idict[token])\n",
        "          else : tmp.append(idict['oov'])\n",
        "        result.append(tmp)\n",
        "          \n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "B0EpO2rQDdsD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Test code**"
      ],
      "metadata": {
        "id": "7FWKWykwK1XL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['I go to school.', 'I LIKE pizza!']\n",
        "t = Tokenizer()"
      ],
      "metadata": {
        "id": "APn8ZfUiJqr3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.preprocessing(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoEK_EG_dpfO",
        "outputId": "ce85f002-7b85-49ad-cb26-4120427d9649"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.fit(sentences)\n",
        "t.word_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nMDq8SIdhmc",
        "outputId": "56afbdda-a085-463c-8bfe-ecadbd754a80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'go': 1, 'i': 2, 'like': 1, 'oov': 0, 'pizza': 1, 'school': 1, 'to': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.transform(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86CNwH7YdoK6",
        "outputId": "4456a95d-caa5-4680-ffcb-e33e89af0a21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 2) TfidfVectorizer 생성하기**"
      ],
      "metadata": {
        "id": "rgfvExJpQPvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cqlajcFHQWvR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    '''\n",
        "    문제 2-1.\n",
        "    '''\n",
        "    N = len(tokenized)\n",
        "    wordset = self.tokenizer.word_dict.copy()\n",
        "    wordset.pop('oov')\n",
        "\n",
        "    idf_dict = dict.fromkeys(wordset, 0)\n",
        "    \n",
        "    for w in wordset:\n",
        "        idf_dict[w] = np.log(N/(1+wordset[w]))\n",
        "    self.idf_matrix = list(idf_dict.values())\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      words = list(self.tokenizer.word_dict.keys())[1:]\n",
        "      token_list = self.tokenizer.preprocessing(sequences)\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      '''\n",
        "      # TF\n",
        "      N = len(tokenized)\n",
        "      TF = []\n",
        "      for i in range(N):\n",
        "        TF.append([])\n",
        "        d = token_list[i]\n",
        "        for j in range(len(words)):\n",
        "          t = words[j]\n",
        "          TF[-1].append(d.count(t))\n",
        "      \n",
        "      self.tfidf_matrix = (np.array(TF) * self.idf_matrix).tolist()\n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "hbjX4JZiQQXB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test code**\n"
      ],
      "metadata": {
        "id": "H8S1_cAUq6RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['I go to school.', 'I LIKE pizza!']\n",
        "t = Tokenizer()\n",
        "tv = TfidfVectorizer(t)\n",
        "tv.fit(sentences)\n",
        "tv.transform(sentences)"
      ],
      "metadata": {
        "id": "kByxnUD7q--e",
        "outputId": "45c5cfc0-0b71-450b-b473-8371d8112bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}